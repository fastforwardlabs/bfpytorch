{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough project/code architecture\n",
    "\n",
    "1. Load MNIST into some kind of native dataformat (i.e. not with a oneliner that gives you a pytorch dataset)\n",
    "1. Load entire MNIST into a dataset, then make that into a dataloader\n",
    "1. Train MNIST with pytorch.  make a note of loss and accuracy. You’ll need to choose an architecture, batch size and number of epochs to train for. \n",
    "    * Get architecture from pytorch docs. Should be simple. \n",
    "    * Number of epochs is “until results get as good as they are going to get”, i.e. by inspection.\n",
    "    * Batch size I got no idea. 1 is probably fine, but feel free to try others.\n",
    "1. Split MNIST into N groups randomly. Create N datasets. Train with federated.py. You should get roughly the final loss and accuracy you saw previously. \n",
    "    * Use the same architecture as before.\n",
    "    * Choose number of rounds same way you chose number of epochs in previous step, i.e. until it converges.\n",
    "    * Number of epochs per round. Maybe try 1, 5 and 10? Go with 1 unless you get very different results.\n",
    "    * Batch size should probably be the same as before.\n",
    "1. Repeat, but split MNIST into N groups with the deck stacked (possibly as extremely as 10 groups each containing data from only one class).\n",
    "\n",
    "### Snippets\n",
    "\n",
    "#### Visualize an array\n",
    "See cell 109 here: https://github.com/williamsmj/pytorch-notes/blob/master/pytorch-60-minute-blitz.ipynb\n",
    "\n",
    "#### Change native data to dataset, change dataset to native data, and loop over dataset doing something other than training\n",
    "See https://github.com/williamsmj/pytorch-notes/blob/master/dataset_manipulation.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './MNIST-data/raw'\n",
    "\n",
    "# location of data and labels\n",
    "test_labels_file = data_path + '/' + 't10k-labels-idx1-ubyte'\n",
    "test_data_file = data_path + '/' + 't10k-images-idx3-ubyte'\n",
    "train_labels_file = data_path + '/' + 'train-labels-idx1-ubyte'\n",
    "train_data_file = data_path + '/' + 'train-images-idx3-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# pytorch datasets that download MNIST set as needed; used only to download files\n",
    "train_dset = dsets.MNIST(root=data_path, download=False, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=False, train=False, transform=trans)\n",
    "\n",
    "#print(\"Training dset:\", train_dset)\n",
    "#print(\"Test dset:\", test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data into numpy arrays\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "# read data and label files into numpy arrays\n",
    "test_data = read_idx(test_data_file)\n",
    "test_labels = read_idx(test_labels_file)\n",
    "train_data = read_idx(train_data_file)\n",
    "train_labels = read_idx(train_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.savetxt(\"./MNIST-data/raw/test_data.csv\", test_data.reshape(len(test_data), 784), delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/test_labels.csv\", test_labels, delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/train_data.csv\", train_data.reshape(len(train_data), 784), delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/train_labels.csv\", train_labels, delimiter=\\',\\')\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save CSV files if needed\n",
    "\n",
    "\"\"\"\n",
    "np.savetxt(\"./MNIST-data/raw/test_data.csv\", test_data.reshape(len(test_data), 784), delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/test_labels.csv\", test_labels, delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/train_data.csv\", train_data.reshape(len(train_data), 784), delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/train_labels.csv\", train_labels, delimiter=',')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# to restore\n",
    "#np.loadtext()\n",
    "#reshape((len(), 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# pick a random sample from the sets of observations\\nimport random\\ntest_sample_num = random.randint(0, len(test_labels))\\ntrain_sample_num = random.randint(0, len(train_labels))\\n\\n# print some arrays to confirm we have data\\nprint(\"Test labels:\", test_labels)\\nprint(\"Test data:\", test_data[test_sample_num])\\nprint(\"Training labels:\", train_labels)\\nprint(\"Training data:\", train_data[train_sample_num])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data in the arrays\n",
    "\n",
    "\"\"\"\n",
    "# pick a random sample from the sets of observations\n",
    "import random\n",
    "test_sample_num = random.randint(0, len(test_labels))\n",
    "train_sample_num = random.randint(0, len(train_labels))\n",
    "\n",
    "# print some arrays to confirm we have data\n",
    "print(\"Test labels:\", test_labels)\n",
    "print(\"Test data:\", test_data[test_sample_num])\n",
    "print(\"Training labels:\", train_labels)\n",
    "print(\"Training data:\", train_data[train_sample_num])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimg = test_data[test_sample_num]\\nlabel = test_labels[test_sample_num]\\nprint('Test sample index: ', test_sample_num)\\nprint('Sample value: ', label)\\nplt.imshow(img)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize some test samples\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_data[test_sample_num]\n",
    "label = test_labels[test_sample_num]\n",
    "print('Test sample index: ', test_sample_num)\n",
    "print('Sample value: ', label)\n",
    "plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Test sample histogram:')\\nplt.hist(test_labels, bins=10)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how is the test data distributed?\n",
    "\n",
    "\"\"\"\n",
    "print('Test sample histogram:')\n",
    "plt.hist(test_labels, bins=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimg = train_data[train_sample_num]\\nlabel = train_labels[train_sample_num]\\nprint('Training sample index: ', train_sample_num)\\nprint('Sample value: ', label)\\nplt.imshow(img)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize a training sample\n",
    "\n",
    "\"\"\"\n",
    "img = train_data[train_sample_num]\n",
    "label = train_labels[train_sample_num]\n",
    "print('Training sample index: ', train_sample_num)\n",
    "print('Sample value: ', label)\n",
    "plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Training data histogram:')\\nplt.hist(train_labels, bins=10)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how is the training data distributed?\n",
    "\n",
    "\"\"\"\n",
    "print('Training data histogram:')\n",
    "plt.hist(train_labels, bins=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58ba15a9c446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# load tensors into datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtrain_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "# load numpy arrays into pytorch Datasets\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# create tensors from np arrays\n",
    "test_data_tensor = torch.from_numpy(test_data)\n",
    "test_labels_tensor = torch.from_numpy(test_labels)\n",
    "train_data_tensor = torch.from_numpy(train_data)\n",
    "train_labels_tensor = torch.from_numpy(train_labels)\n",
    "\n",
    "\n",
    "# TODO: TRANSFORM THE DATA WHEN THE SETS ARE CREATED\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\"\"\"\n",
    "\n",
    "# load tensors into datasets\n",
    "test_dset = TensorDataset(test_data_tensor, test_labels_tensor, transform=trans)\n",
    "train_dset = TensorDataset(train_data_tensor, train_labels_tensor, transform=trans)\n",
    "\n",
    "print(\"test_dset:\",\n",
    "      test_dset,\n",
    "      len(test_dset), test_dset[0][0].size(), type(test_dset[0][0])\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1\n",
    "test_dloader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"test_dloader\", len(test_dloader),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up network\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "## network\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\"\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = MLPNet()\n",
    "\n",
    "learning_rate = 0.0003\n",
    "momentum = 0.5\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    # train\n",
    "    for batch_idx, (x, target) in enumerate(train_dloader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        train_loss_history.append(loss.item())\n",
    "        avg_loss = avg_loss * 0.99 + loss.item() * 0.01\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "        if (batch_idx+1) % 2000 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, avg_loss))\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# ADD TESTING AFTER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD PLOTTING\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss_history, label=\"train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## training\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    # training\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_dloader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        train_loss_history.append(loss.item())\n",
    "        ave_loss = ave_loss * 0.99 + loss.item() * 0.01\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_dloader):\n",
    "        with torch.no_grad():\n",
    "            x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        test_loss_history.append(loss.item())\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.99 + loss.item() * 0.01\n",
    "        \n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss_history, label=\"train\");\n",
    "ax.plot(test_loss_history, label=\"test\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
